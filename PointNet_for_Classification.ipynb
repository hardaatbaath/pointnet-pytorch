{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVfkY8sqvSOraBM3WK8tv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardaatbaath/pointnet_pytorch/blob/main/PointNet_for_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics open3d -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WGEClgOtCrm",
        "outputId": "d2fb21fd-ded9-41ca-d9d7-4cf0ae103065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex-6jL3vsdRG"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
        "import open3d as o3\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP for supressing pytorch user warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "yFXk4bXDs2eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {
        "id": "ImtacS8Mt3_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this only if you don't already have the Dataset\n",
        "!wget -nv https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate\n",
        "!unzip shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
        "!rm shapenetcore_partanno_segmentation_benchmark_v0.zip"
      ],
      "metadata": {
        "id": "Sssfnrt3tgeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General parameters\n",
        "NUM_TRAIN_POINTS = 2500\n",
        "NUM_TEST_POINTS = 10000\n",
        "NUM_CLASSES = 16\n",
        "ROOT = r'C:\\Users\\itber\\Documents\\datasets\\shapenetcore_partanno_segmentation_benchmark_v0'\n",
        "\n",
        "# model hyperparameters\n",
        "GLOBAL_FEATS = 1024\n",
        "\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "zaTR6HlOtirp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get class - label mappings\n",
        "CATEGORIES = {\n",
        "    'Airplane': 0,\n",
        "    'Bag': 1,\n",
        "    'Cap': 2,\n",
        "    'Car': 3,\n",
        "    'Chair': 4,\n",
        "    'Earphone': 5,\n",
        "    'Guitar': 6,\n",
        "    'Knife': 7,\n",
        "    'Lamp': 8,\n",
        "    'Laptop': 9,\n",
        "    'Motorbike': 10,\n",
        "    'Mug': 11,\n",
        "    'Pistol': 12,\n",
        "    'Rocket': 13,\n",
        "    'Skateboard': 14,\n",
        "    'Table': 15}\n",
        "\n",
        "# Simple point cloud coloring mapping for part segmentation\n",
        "def read_pointnet_colors(seg_labels):\n",
        "    map_label_to_rgb = {\n",
        "        1: [0, 255, 0],\n",
        "        2: [0, 0, 255],\n",
        "        3: [255, 0, 0],\n",
        "        4: [255, 0, 255],  # purple\n",
        "        5: [0, 255, 255],  # cyan\n",
        "        6: [255, 255, 0],  # yellow\n",
        "    }\n",
        "    colors = np.array([map_label_to_rgb[label] for label in seg_labels])\n",
        "    return colors"
      ],
      "metadata": {
        "id": "D1-Q87GjtxDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from shapenet_dataset import ShapenetDataset\n",
        "\n",
        "# train Dataset & DataLoader\n",
        "train_dataset = ShapenetDataset(ROOT, npoints=NUM_TRAIN_POINTS, split='train', classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Validation Dataset & DataLoader\n",
        "valid_dataset = ShapenetDataset(ROOT, npoints=NUM_TRAIN_POINTS, split='valid', classification=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# test Dataset & DataLoader\n",
        "test_dataset = ShapenetDataset(ROOT, npoints=NUM_TEST_POINTS, split='test', classification=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# test Dataset  (segmentation version for display)\n",
        "test_sample_dataset = ShapenetDataset(ROOT, npoints=NUM_TEST_POINTS, split='test',\n",
        "                                      classification=False, normalize=False)"
      ],
      "metadata": {
        "id": "bjv6HEI-tzZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data"
      ],
      "metadata": {
        "id": "TFNteFget8xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dataset = ShapenetDataset(ROOT, npoints=20000, split='train',\n",
        "                                 classification=False, normalize=False)"
      ],
      "metadata": {
        "id": "HNKE_PDRt8mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points, seg = sample_dataset[4000]\n",
        "\n",
        "pcd = o3.geometry.PointCloud()\n",
        "pcd.points = o3.utility.Vector3dVector(points)\n",
        "pcd.colors = o3.utility.Vector3dVector(read_pointnet_colors(seg.numpy()))\n",
        "\n",
        "o3.visualization.draw_plotly([pcd])"
      ],
      "metadata": {
        "id": "30z_vq93uAth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_targets = []\n",
        "for (_, targets) in train_dataloader:\n",
        "    total_train_targets += targets.reshape(-1).numpy().tolist()\n",
        "\n",
        "total_train_targets = np.array(total_train_targets)"
      ],
      "metadata": {
        "id": "Ae3WmNwNuD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_bins = np.bincount(total_train_targets)\n",
        "\n",
        "plt.bar(list(CATEGORIES.keys()), class_bins,\n",
        "             color=mpl.cm.tab20(np.arange(0, NUM_CLASSES)),\n",
        "             edgecolor='black')\n",
        "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
        "plt.ylabel('Counts', size=12)\n",
        "plt.title('Train Class Frequencies', size=14, pad=20);"
      ],
      "metadata": {
        "id": "8qBR-NZGuFfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hardaatbaath/pointnet_pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UNZK8YYuQIc",
        "outputId": "0b567ad2-3763-40a6-f642-7e8975d92d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pointnet_pytorch'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 28 (delta 9), reused 19 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (28/28), 6.35 KiB | 6.35 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pointnet_pytorch.point_net import PointNetClassHead\n",
        "\n",
        "points, targets = next(iter(train_dataloader))\n",
        "\n",
        "classifier = PointNetClassHead(k=NUM_CLASSES, num_global_feats=GLOBAL_FEATS)\n",
        "out, _, _ = classifier(points.transpose(2, 1))\n",
        "print(f'Class output shape: {out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QxbtiK13uHSK",
        "outputId": "5493193f-b20c-4db3-ecdd-54e9d00d89ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1a146f2c8d78>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpointnet_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPointNetClassHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointNetClassHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_global_feats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGLOBAL_FEATS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "wi6nlXwquPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from point_net_loss import PointNetLoss\n",
        "\n",
        "EPOCHS = 100\n",
        "LR = 0.0001\n",
        "REG_WEIGHT = 0.001\n",
        "\n",
        "# use inverse class weighting\n",
        "# alpha = 1 / class_bins\n",
        "# alpha = (alpha/alpha.max())\n",
        "\n",
        "# manually downweight the high frequency classes\n",
        "alpha = np.ones(NUM_CLASSES)\n",
        "alpha[0] = 0.5  # airplane\n",
        "alpha[4] = 0.5  # chair\n",
        "alpha[-1] = 0.5 # table\n",
        "\n",
        "gamma = 2\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01,\n",
        "                                              step_size_up=2000, cycle_momentum=False)\n",
        "criterion = PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=REG_WEIGHT).to(DEVICE)\n",
        "\n",
        "classifier = classifier.to(DEVICE)"
      ],
      "metadata": {
        "id": "r7SkgiYjwbXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
      ],
      "metadata": {
        "id": "K9vp0AeAwdeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test(classifier, dataloader, num_batch, epoch, split='train'):\n",
        "    ''' Function to train or test the model '''\n",
        "    _loss = []\n",
        "    _accuracy = []\n",
        "    _mcc = []\n",
        "\n",
        "    # return total targets and predictions for test case only\n",
        "    total_test_targets = []\n",
        "    total_test_preds = []\n",
        "    for i, (points, targets) in enumerate(dataloader, 0):\n",
        "\n",
        "        points = points.transpose(2, 1).to(DEVICE)\n",
        "        targets = targets.squeeze().to(DEVICE)\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get predicted class logits\n",
        "        preds, _, A = classifier(points)\n",
        "\n",
        "        # get loss and perform backprop\n",
        "        loss = criterion(preds, targets, A)\n",
        "\n",
        "        if split == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # get class predictions\n",
        "        pred_choice = torch.softmax(preds, dim=1).argmax(dim=1)\n",
        "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
        "        accuracy = correct.item()/float(BATCH_SIZE)\n",
        "        mcc = mcc_metric(preds, targets)\n",
        "\n",
        "        # update epoch loss and accuracy\n",
        "        _loss.append(loss.item())\n",
        "        _accuracy.append(accuracy)\n",
        "        _mcc.append(mcc.item())\n",
        "\n",
        "        # add to total targets/preds\n",
        "        if split == 'test':\n",
        "            total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
        "            total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'\\t [{epoch}: {i}/{num_batch}] ' \\\n",
        "                  + f'{split} loss: {loss.item():.4f} ' \\\n",
        "                  + f'accuracy: {accuracy:.4f} mcc: {mcc:.4f}')\n",
        "\n",
        "    epoch_loss = np.mean(_loss)\n",
        "    epoch_accuracy = np.mean(_accuracy)\n",
        "    epoch_mcc = np.mean(_mcc)\n",
        "\n",
        "    print(f'Epoch: {epoch} - {split} Loss: {epoch_loss:.4f} ' \\\n",
        "          + f'- {split} Accuracy: {epoch_accuracy:.4f} ' \\\n",
        "          + f'- {split} MCC: {epoch_mcc:.4f}')\n",
        "\n",
        "    if split == 'test':\n",
        "        return epoch_loss, epoch_accuracy, epoch_mcc, total_test_targets, total_test_preds\n",
        "    else:\n",
        "        return epoch_loss, epoch_accuracy, epoch_mcc"
      ],
      "metadata": {
        "id": "H4YDt3FmwhnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stuff for training\n",
        "num_train_batch = int(np.ceil(len(train_dataset)/BATCH_SIZE))\n",
        "num_valid_batch = int(np.ceil(len(valid_dataset)/BATCH_SIZE))\n",
        "\n",
        "# store best validation mcc above 0.\n",
        "best_mcc = 0.\n",
        "\n",
        "# lists to store metrics (loss, accuracy, mcc)\n",
        "train_metrics = []\n",
        "valid_metrics = []\n",
        "\n",
        "# TRAIN ON EPOCHS\n",
        "for epoch in range(1, EPOCHS):\n",
        "\n",
        "    ## train loop\n",
        "    classifier = classifier.train()\n",
        "\n",
        "    # train\n",
        "    _train_metrics = train_test(classifier, train_dataloader,\n",
        "                                num_train_batch, epoch,\n",
        "                                split='train')\n",
        "    train_metrics.append(_train_metrics)\n",
        "\n",
        "\n",
        "    # pause to cool down\n",
        "    time.sleep(4)\n",
        "\n",
        "    ## validation loop\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # place model in evaluation mode\n",
        "        classifier = classifier.eval()\n",
        "\n",
        "        # validate\n",
        "        _valid_metrics = train_test(classifier, valid_dataloader,\n",
        "                                    num_valid_batch, epoch,\n",
        "                                    split='valid')\n",
        "        valid_metrics.append(_valid_metrics)\n",
        "\n",
        "        # pause to cool down\n",
        "        time.sleep(4)\n",
        "\n",
        "    # save model if necessary\n",
        "    if valid_metrics[-1][-1] >= best_mcc:\n",
        "        best_mcc = valid_metrics[-1][-1]\n",
        "        torch.save(classifier.state_dict(), 'trained_models/cls_focal_clr_2/cls_model_%d.pth' % epoch)"
      ],
      "metadata": {
        "id": "6hzjazP2wksU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_names = ['loss', 'accuracy', 'mcc']\n",
        "_, ax = plt.subplots(len(metric_names), 1, figsize=(8, 6))\n",
        "\n",
        "for i, m in enumerate(metric_names):\n",
        "    ax[i].set_title(m)\n",
        "    ax[i].plot(train_metrics[:, i], label='train')\n",
        "    ax[i].plot(valid_metrics[:, i], label='valid')\n",
        "    ax[i].legend()\n",
        "\n",
        "plt.subplots_adjust(wspace=0., hspace=0.35)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xq3jCQ1vwnDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = 'trained_models/cls_focal_clr/cls_model_35.pth'\n",
        "\n",
        "classifier = PointNetClassHead(num_points=NUM_TEST_POINTS, num_global_feats=GLOBAL_FEATS, k=NUM_CLASSES).to(DEVICE)\n",
        "classifier.load_state_dict(torch.load(MODEL_PATH))\n",
        "classifier.eval();"
      ],
      "metadata": {
        "id": "Ap8lvNErwp0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_batch = int(np.ceil(len(test_dataset)/BATCH_SIZE))\n",
        "\n",
        "with torch.no_grad():\n",
        "    epoch_loss, \\\n",
        "    epoch_accuracy, \\\n",
        "    epoch_mcc, \\\n",
        "    total_test_targets, \\\n",
        "    total_test_preds = train_test(classifier, test_dataloader,\n",
        "                              num_test_batch, epoch=1,\n",
        "                              split='test')"
      ],
      "metadata": {
        "id": "sY3H7mqnwrce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Loss: {epoch_loss:.4f} ' \\\n",
        "      f'- Test Accuracy: {epoch_accuracy:.4f} ' \\\n",
        "      f'- Test MCC: {epoch_mcc:.4f}')"
      ],
      "metadata": {
        "id": "PtvKemCzwtJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
        "                              columns=list(CATEGORIES.keys()),\n",
        "                              index=list(CATEGORIES.keys()))\n",
        "\n",
        "test_confusion"
      ],
      "metadata": {
        "id": "RMVxbWulwvPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "torch.cuda.empty_cache() # release GPU memory\n",
        "\n",
        "# get random sample from test data\n",
        "random_idx = randrange(len(test_sample_dataset))\n",
        "points, seg = test_sample_dataset.__getitem__(random_idx)\n",
        "\n",
        "# normalize points\n",
        "norm_points = test_sample_dataset.normalize_points(points)\n",
        "\n",
        "with torch.no_grad():\n",
        "    norm_points = norm_points.unsqueeze(0).transpose(2, 1).to(DEVICE)\n",
        "    targets = targets.squeeze().to(DEVICE)\n",
        "\n",
        "    preds, crit_idxs, _ = classifier(norm_points)\n",
        "    preds = torch.softmax(preds, dim=1)\n",
        "    pred_choice = preds.squeeze().argmax()"
      ],
      "metadata": {
        "id": "IK6Vg5QUwxis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class = list(CATEGORIES.keys())[pred_choice.cpu().numpy()]\n",
        "pred_prob = preds[0, pred_choice]\n",
        "print(f'The predicted class is: {pred_class}, with probability: {pred_prob}')"
      ],
      "metadata": {
        "id": "dhTmpKqAwx_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(CATEGORIES.values()), preds.cpu().numpy()[0]);\n",
        "plt.xticks(list(CATEGORIES.values()), list(CATEGORIES.keys()), rotation=90)\n",
        "plt.title('Predicted Classes')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Probabilities');"
      ],
      "metadata": {
        "id": "-Ro4zEnbw0Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcd = o3.geometry.PointCloud()\n",
        "pcd.points = o3.utility.Vector3dVector(points.cpu().numpy())\n",
        "pcd.colors = o3.utility.Vector3dVector(read_pointnet_colors(seg.numpy()))\n",
        "\n",
        "o3.visualization.draw_plotly([pcd])"
      ],
      "metadata": {
        "id": "luyKpQhaw49K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critical_points = points[crit_idxs.squeeze(), :]\n",
        "critical_point_colors = read_pointnet_colors(seg.numpy())[crit_idxs.cpu().squeeze(), :]\n",
        "\n",
        "pcd = o3.geometry.PointCloud()\n",
        "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
        "pcd.colors = o3.utility.Vector3dVector(critical_point_colors)"
      ],
      "metadata": {
        "id": "k4cB5AZiw88l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}