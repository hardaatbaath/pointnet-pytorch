{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "import open3d as o3\n",
    "# from open3d import JVisualizer # For Colab Visualization\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ROOT = r'content\\datasets\\S3DIS\\Stanford3dDataset_v1.2_Reduced_Partitioned_Aligned_Version_1m'\n",
    "\n",
    "# feature selection hyperparameters\n",
    "NUM_TRAIN_POINTS = 4096 # train/valid points\n",
    "NUM_TEST_POINTS = 15000\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'ceiling'  : 0, \n",
    "    'floor'    : 1, \n",
    "    'wall'     : 2, \n",
    "    'beam'     : 3, \n",
    "    'column'   : 4, \n",
    "    'window'   : 5,\n",
    "    'door'     : 6, \n",
    "    'table'    : 7, \n",
    "    'chair'    : 8, \n",
    "    'sofa'     : 9, \n",
    "    'bookcase' : 10, \n",
    "    'board'    : 11,\n",
    "    'stairs'   : 12,\n",
    "    'clutter'  : 13\n",
    "}\n",
    "\n",
    "# unique color map generated via\n",
    "# https://mokole.com/palette.html\n",
    "COLOR_MAP = {\n",
    "    0  : (47, 79, 79),    # ceiling - darkslategray\n",
    "    1  : (139, 69, 19),   # floor - saddlebrown\n",
    "    2  : (34, 139, 34),   # wall - forestgreen\n",
    "    3  : (75, 0, 130),    # beam - indigo\n",
    "    4  : (255, 0, 0),     # column - red \n",
    "    5  : (255, 255, 0),   # window - yellow\n",
    "    6  : (0, 255, 0),     # door - lime\n",
    "    7  : (0, 255, 255),   # table - aqua\n",
    "    8  : (0, 0, 255),     # chair - blue\n",
    "    9  : (255, 0, 255),   # sofa - fuchsia\n",
    "    10 : (238, 232, 170), # bookcase - palegoldenrod\n",
    "    11 : (100, 149, 237), # board - cornflower\n",
    "    12 : (255, 105, 180), # stairs - hotpink\n",
    "    13 : (0, 0, 0)        # clutter - black\n",
    "}\n",
    "\n",
    "v_map_colors = np.vectorize(lambda x : COLOR_MAP[x])\n",
    "\n",
    "NUM_CLASSES = len(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from s3dis_dataset import S3DIS\n",
    "\n",
    "# get datasets\n",
    "s3dis_train = S3DIS(ROOT, area_nums='1-4', npoints=NUM_TRAIN_POINTS, r_prob=0.25)\n",
    "s3dis_valid = S3DIS(ROOT, area_nums='5', npoints=NUM_TRAIN_POINTS, r_prob=0.)\n",
    "s3dis_test = S3DIS(ROOT, area_nums='6', split='test', npoints=NUM_TEST_POINTS)\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader = DataLoader(s3dis_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(s3dis_valid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(s3dis_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, targets = s3dis_train[1000]\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets)).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_targets = []\n",
    "for (_, targets) in train_dataloader:\n",
    "    total_train_targets += targets.reshape(-1).numpy().tolist()\n",
    "\n",
    "total_train_targets = np.array(total_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), class_bins, \n",
    "             color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "             edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category (Training - Areas 1-4)', size=14, pad=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point_net import PointNetSegHead\n",
    "\n",
    "points, targets = next(iter(train_dataloader))\n",
    "seg_model = PointNetSegHead(num_points=NUM_TRAIN_POINTS, m=NUM_CLASSES)\n",
    "out, _, _ = seg_model(points.transpose(2, 1))\n",
    "print(f'Seg shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'seg_balanced/seg_model_15.pth'\n",
    "\n",
    "# seg_model = PointNetSegHead(num_points=NUM_TRAIN_POINTS, m=NUM_CLASSES).to(DEVICE)\n",
    "# seg_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from point_net_loss import PointNetSegLoss\n",
    "\n",
    "EPOCHS = 100\n",
    "LR = 0.0001\n",
    "\n",
    "# use inverse class weighting\n",
    "# alpha = 1 / class_bins\n",
    "# alpha = (alpha/alpha.max())\n",
    "\n",
    "# manually set alpha weights\n",
    "alpha = np.ones(len(CATEGORIES))\n",
    "alpha[0:3] *= 0.25 # balance background classes\n",
    "alpha[-1] *= 0.75  # balance clutter class\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "optimizer = optim.Adam(seg_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, \n",
    "                                              step_size_up=1000, cycle_momentum=False)\n",
    "criterion = PointNetSegLoss(alpha=alpha, gamma=gamma, dice=True).to(DEVICE)\n",
    "\n",
    "seg_model = seg_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(targets, predictions):\n",
    "\n",
    "    targets = targets.reshape(-1)\n",
    "    predictions = predictions.reshape(-1)\n",
    "\n",
    "    intersection = torch.sum(predictions == targets) # true positives\n",
    "    union = len(predictions) + len(targets) - intersection\n",
    "\n",
    "    return intersection / union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store best validation iou\n",
    "best_iou = 0.6\n",
    "best_mcc = 0.6\n",
    "\n",
    "# lists to store metrics\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "train_mcc = []\n",
    "train_iou = []\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "valid_mcc = []\n",
    "valid_iou = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff for training\n",
    "num_train_batch = int(np.ceil(len(s3dis_train)/BATCH_SIZE))\n",
    "num_valid_batch = int(np.ceil(len(s3dis_valid)/BATCH_SIZE))\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # place model in training mode\n",
    "    seg_model = seg_model.train()\n",
    "    _train_loss = []\n",
    "    _train_accuracy = []\n",
    "    _train_mcc = []\n",
    "    _train_iou = []\n",
    "    for i, (points, targets) in enumerate(train_dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get predicted class logits\n",
    "        preds, _, _ = seg_model(points)\n",
    "\n",
    "        # get class predictions\n",
    "        pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "        # get loss and perform backprop\n",
    "        loss = criterion(preds, targets, pred_choice) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # update learning rate\n",
    "        \n",
    "        # get metrics\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "        mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "        iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        _train_loss.append(loss.item())\n",
    "        _train_accuracy.append(accuracy)\n",
    "        _train_mcc.append(mcc.item())\n",
    "        _train_iou.append(iou.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_train_batch}] ' \\\n",
    "                  + f'train loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} ' \\\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "        \n",
    "    train_loss.append(np.mean(_train_loss))\n",
    "    train_accuracy.append(np.mean(_train_accuracy))\n",
    "    train_mcc.append(np.mean(_train_mcc))\n",
    "    train_iou.append(np.mean(_train_iou))\n",
    "\n",
    "    print(f'Epoch: {epoch} - Train Loss: {train_loss[-1]:.4f} ' \\\n",
    "          + f'- Train Accuracy: {train_accuracy[-1]:.4f} ' \\\n",
    "          + f'- Train MCC: {train_mcc[-1]:.4f} ' \\\n",
    "          + f'- Train IOU: {train_iou[-1]:.4f}')\n",
    "\n",
    "    # pause to cool down\n",
    "    time.sleep(4)\n",
    "\n",
    "    # get test results after each epoch\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # place model in evaluation mode\n",
    "        seg_model = seg_model.eval()\n",
    "\n",
    "        _valid_loss = []\n",
    "        _valid_accuracy = []\n",
    "        _valid_mcc = []\n",
    "        _valid_iou = []\n",
    "        for i, (points, targets) in enumerate(valid_dataloader, 0):\n",
    "\n",
    "            points = points.transpose(2, 1).to(DEVICE)\n",
    "            targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "            preds, _, A = seg_model(points)\n",
    "            pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "            loss = criterion(preds, targets, pred_choice) \n",
    "\n",
    "            # get metrics\n",
    "            correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "            accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "            mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "            iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "            # update epoch loss and accuracy\n",
    "            _valid_loss.append(loss.item())\n",
    "            _valid_accuracy.append(accuracy)\n",
    "            _valid_mcc.append(mcc.item())\n",
    "            _valid_iou.append(iou.item())\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'\\t [{epoch}: {i}/{num_valid_batch}] ' \\\n",
    "                  + f'valid loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} '\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "        \n",
    "        valid_loss.append(np.mean(_valid_loss))\n",
    "        valid_accuracy.append(np.mean(_valid_accuracy))\n",
    "        valid_mcc.append(np.mean(_valid_mcc))\n",
    "        valid_iou.append(np.mean(_valid_iou))\n",
    "        print(f'Epoch: {epoch} - Valid Loss: {valid_loss[-1]:.4f} ' \\\n",
    "              + f'- Valid Accuracy: {valid_accuracy[-1]:.4f} ' \\\n",
    "              + f'- Valid MCC: {valid_mcc[-1]:.4f} ' \\\n",
    "              + f'- Valid IOU: {valid_iou[-1]:.4f}')\n",
    "\n",
    "\n",
    "        # pause to cool down\n",
    "        time.sleep(4)\n",
    "\n",
    "    # save best models\n",
    "    if valid_iou[-1] >= best_iou:\n",
    "        best_iou = valid_iou[-1]\n",
    "        torch.save(seg_model.state_dict(), f'trained_models/seg_focal_dice_iou_rot_clr/seg_model_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(8, 5))\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), train_loss, label='train')\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), valid_loss, label='valid')\n",
    "ax[0].set_title('loss')\n",
    "\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), train_accuracy)\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), valid_accuracy)\n",
    "ax[1].set_title('accuracy')\n",
    "\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), train_mcc)\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), valid_mcc)\n",
    "ax[2].set_title('mcc')\n",
    "\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), train_iou)\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), valid_iou)\n",
    "ax[3].set_title('iou')\n",
    "\n",
    "fig.legend(loc='upper right')\n",
    "plt.subplots_adjust(wspace=0., hspace=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'trained_models/seg_focal_dice_iou_rot/seg_model_68.pth'\n",
    "\n",
    "model = PointNetSegHead(num_points=NUM_TEST_POINTS, m=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_batch = int(np.ceil(len(s3dis_test)/BATCH_SIZE))\n",
    "\n",
    "total_test_targets = []\n",
    "total_test_preds = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # place model in evaluation mode\n",
    "    model = model.eval()\n",
    "\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    test_mcc = []\n",
    "    test_iou = []\n",
    "    for i, (points, targets) in enumerate(test_dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "        preds, _, A = model(points)\n",
    "        pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "        loss = criterion(preds, targets, pred_choice)\n",
    "\n",
    "        # get metrics\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct/float(BATCH_SIZE*NUM_TEST_POINTS)\n",
    "        mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "        iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        test_loss.append(loss.item())\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_mcc.append(mcc.item())\n",
    "        test_iou.append(iou.item())\n",
    "\n",
    "        # add to total targets/preds\n",
    "        total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
    "        total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'\\t [{i}/{num_test_batch}] ' \\\n",
    "                  + f'test loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} ' \\\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test results\n",
    "print(f'Test Loss: {np.mean(test_loss):.4f} ' \\\n",
    "        + f'- Test Accuracy: {np.mean(test_accuracy):.4f} ' \\\n",
    "        + f'- Test MCC: {np.mean(test_mcc):.4f} ' \\\n",
    "        + f'- Test IOU: {np.mean(test_iou):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_targets = np.array(total_test_targets)\n",
    "total_test_preds = np.array(total_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
    "                              columns=list(CATEGORIES.keys()),\n",
    "                              index=list(CATEGORIES.keys()))\n",
    "\n",
    "test_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "# place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "# Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "norm_points = norm_points - norm_points.min(axis=1)[0].unsqueeze(1)\n",
    "norm_points /= norm_points.max(axis=1)[0].unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    # run inference\n",
    "    preds, _, _ = model(norm_points)\n",
    "\n",
    "    # get metrics\n",
    "    pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "    loss = criterion(preds, targets, pred_choice)\n",
    "    correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "    accuracy = correct/float(points.shape[0]*NUM_TEST_POINTS)\n",
    "    mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "    iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "print(f'Loss: {loss:.4f} - Accuracy: {accuracy:.4f} - MCC: {mcc:.4f} - IOU: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted full point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full_predicted.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "pcd_1 = o3.geometry.PointCloud()\n",
    "pcd_1.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd_1.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "# display predicted full point cloud\n",
    "pcd_2 = o3.geometry.PointCloud()\n",
    "pcd_2.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T + torch.Tensor([5, 0, 0]))\n",
    "pcd_2.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw([pcd_1] + [pcd_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "# place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "# Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "norm_points = norm_points - norm_points.min(axis=1)[0].unsqueeze(1)\n",
    "norm_points /= norm_points.max(axis=1)[0].unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    # run inference to get critical indexes\n",
    "    _, crit_idxs, _ = model(norm_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = []\n",
    "crit_pcds = []\n",
    "for i in range(points.shape[0]):\n",
    "    \n",
    "    pts = points[i, :]\n",
    "    cdx = crit_idxs[i, :]\n",
    "    tgt = targets[i, :]\n",
    "\n",
    "    # get full point clouds\n",
    "    pcd = o3.geometry.PointCloud()\n",
    "    pcd.points = o3.utility.Vector3dVector(pts)\n",
    "    pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(tgt)).T/255)\n",
    "\n",
    "    # get critical set point clouds\n",
    "    critical_points = pts[cdx, :]\n",
    "    critical_point_colors = np.vstack(v_map_colors(tgt[cdx])).T/255\n",
    "\n",
    "    crit_pcd = o3.geometry.PointCloud()\n",
    "    crit_pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "    crit_pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "    pcds.append(pcd)\n",
    "    crit_pcds.append(crit_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(pcds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('full_set.pcd', pcds_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = points[0, :].to('cpu')\n",
    "cdx = crit_idxs[0, :].to('cpu')\n",
    "tgt = targets[0, :].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_points = pts[cdx, :]\n",
    "critical_point_colors = np.vstack(v_map_colors(tgt[cdx])).T/255\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "# o3.visualization.draw_plotly([pcd])\n",
    "# draw(pcd, point_size=5) # does not work in Colab\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draw(crit_pcds, point_size=5)\n",
    "\n",
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('critical_set.pcd', pcds_combined)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
